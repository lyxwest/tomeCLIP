{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/anaconda3/envs/tome/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import tome\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to be whatever device you want to benchmark on\n",
    "# If you don't have a GPU, you can use \"cpu\" but you probably want to set the # runs to be lower\n",
    "device = \"cuda:0\"\n",
    "runs = 50\n",
    "batch_size = 256  # Lower this if you don't have that much memory\n",
    "input_size = [3,224,224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"dog\", \"cat\", \"wolf\", \"people\", \"stone\", \"grass\"]\n",
    "\n",
    "text_dict = torch.cat([clip.tokenize(c) for c in classes]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 50/50 [03:41<00:00,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 57.81 im/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline benchmark\n",
    "model = model.float()\n",
    "baseline_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size,\n",
    "    text = text_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tome.patch.clip import patch_openclip\n",
    "patch_openclip(model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:   0%|          | 0/50 [00:00<?, ?it/s]/home/lyx/code/tome/tome/merge.py:75: UserWarning: scatter_reduce() is in beta and the API may change at any time. (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1664766666830/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1550.)\n",
      "  dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode)\n",
      "Benchmarking: 100%|██████████| 50/50 [03:03<00:00,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 69.69 im/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tome_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size,\n",
    "    text = text_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 50/50 [03:02<00:00,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 69.83 im/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.r = 16\n",
    "tome_throughput = tome.utils.benchmark(\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    runs=runs,\n",
    "    batch_size=batch_size,\n",
    "    input_size=input_size,\n",
    "    text = text_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ToMeAttention' object has no attribute 'embed_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpatch_openclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/tome/tome/patch/clip.py:207\u001b[0m, in \u001b[0;36mpatch_openclip\u001b[0;34m(model, ratio, trace_source, prop_attn)\u001b[0m\n\u001b[1;32m    205\u001b[0m resblock\u001b[38;5;241m.\u001b[39m_tome_info \u001b[38;5;241m=\u001b[39m vision_model\u001b[38;5;241m.\u001b[39m_tome_info\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# print(resblock.attn.embed_dim)\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m attn \u001b[38;5;241m=\u001b[39m ToMeAttention(\u001b[43mresblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m, resblock\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mnum_heads, qkv_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    208\u001b[0m _, device \u001b[38;5;241m=\u001b[39m convert_attention_block(resblock\u001b[38;5;241m.\u001b[39mattn, attn)\n\u001b[1;32m    209\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/tome/lib/python3.8/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ToMeAttention' object has no attribute 'embed_dim'"
     ]
    }
   ],
   "source": [
    "patch_openclip(model, 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
